{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datavalanche\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our project is to highlight the causes of avalanches which kill around 30 people per year in Switzerland. To do so we plan to use public data on mortal avalanche accidents available on the Schnee- und Lawinenforschung's website, we will also use open weather data in Switzerland.\n",
    "\n",
    "The notebook will be structured as follow:\n",
    "\n",
    "    1) Data Wrangling and Exploratory Data Analysis\n",
    "    2) Map Vizualisations\n",
    "    3) Causes of avalanches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Wrangling and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis of the SLF deadly avalanche dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import useful librairies\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import date\n",
    "import folium\n",
    "from folium import plugins\n",
    "%aimport helpers\n",
    "import calendar\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "#import pygrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that is used below (`data/avalanches.csv`) is the data that is published at http://www.slf.ch/praevention/lawinenunfaelle/unfaelle_langj/index_EN\n",
    "\n",
    "It is obtained through the script `avalanche_data_processor.js`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read the slf data\n",
    "data = pd.read_csv('data/avalanches.csv', parse_dates=['date_posix_ts'], date_parser=helpers.parsedate)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many deadly avalanches occurred each year during the last 20 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avalanches_per_winter = data.groupby('winter').count()['canton']\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Avalanches per winter', fontsize=14, fontweight='bold')\n",
    "avalanches_per_winter.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot avalanches per month\n",
    "data['month'] = data['date_posix_ts'].map(lambda x: x.month)\n",
    "avalanches_per_month = data.groupby('month').count()['canton']\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Avalanches per month', fontsize=14, fontweight='bold')\n",
    "avalanches_per_month.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the number of avalanches per month per winter\n",
    "avalanches_per_winter_and_month = data.groupby(['winter', 'month']).count()[['canton']]\n",
    "avalanches_per_winter_and_month.columns = ['count']\n",
    "avalanches_per_winter_and_month.head(18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let's now look at the aspect of the slopes where the avalanches occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avalanches_per_aspect = data.groupby('aspect_string').count()['canton']\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Avalanches per slopeÂ´s orientation' , fontsize=14, fontweight='bold')\n",
    "avalanches_per_aspect.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that north facing slopes are more dangerous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The danger level is define as follow: http://www.slf.ch/schneeinfo/zusatzinfos/lawinenskala-europa/index_EN\n",
    "avalanches_per_danger = data.groupby('danger_level').count()['canton']\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Avalanches per danger level', fontsize=14, fontweight='bold')\n",
    "avalanches_per_danger.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the distribution of avalanche starting point elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Mean avalanche start point elevation:', data['elevation'].mean())\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Avalanches per elevation', fontsize=14, fontweight='bold')\n",
    "data['elevation'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations and Remarks\n",
    "\n",
    "It is important to note that the avalanches that are present in the dataset are the ones that have caught at least one person.\n",
    "We can see that on average every winter there are about 18 avalanches which are recorded.\n",
    "These avalanches occurs mainly between Decemember and April, whith February the worst month, which recorded more than 100 avalanches these last 20 years. We can also see that most avalanches occur on faces that are facing North (N, NE and NW). \n",
    "The distribution of the avalanches per elevation looks like a Guassian with mean 2500. Most of the time the danger level is 3 when an avalanch occurs, the fact there are less avalanch with higher risk probably due to the fact that these risks happen less frequently and also people will be more inclined to not go in dangerous places when the risk is high enough. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2) Map Vizualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part of the notebook we will do some vizualisations about the data on a map\n",
    "Note that the markers on the map allow you to view the geographic location in 3D, but it seems like the feature doesn't work quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get number of avalanches per canton\n",
    "missing_cantons = ['ZH', 'ZG', 'SO', 'BS', 'BL', 'SH', 'JU', 'GE', 'NE','TG', 'AR', 'AG']\n",
    "avalanches_per_canton = data.groupby('canton').count()[['winter']]\n",
    "avalanches_per_canton.columns = [['count']]\n",
    "# Apply a log scale\n",
    "avalanches_per_canton['count'] = avalanches_per_canton['count'].apply(lambda x: 0 if x == 0 else np.log10(x))\n",
    "avalanches_per_canton = avalanches_per_canton.append(pd.DataFrame({'count':0}, index=missing_cantons))\n",
    "avalanches_per_canton.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the stupid coordinate system in latitude/longitude\n",
    "for i in data.index:\n",
    "    x = data.starting_zone_X[i]\n",
    "    y = data.starting_zone_Y[i]\n",
    "    lat = helpers.CHtoWGSlat(x, y)\n",
    "    lng = helpers.CHtoWGSlng(x, y)\n",
    "    data.set_value(i, 'lat', lat)\n",
    "    data.set_value(i, 'lon', lng)\n",
    "    \n",
    "data.drop(['starting_zone_X','starting_zone_Y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the number of avalanches per community\n",
    "d2 = data[['community', 'lat', 'lon']]\n",
    "cnt = d2.groupby('community').count()[['lat']]\n",
    "cnt.columns = ['count']\n",
    "avalanches_per_community = pd.concat([d2.groupby('community').first(), cnt], axis=1)\n",
    "avalanches_per_community.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topo_path = r'ch-cantons.topojson.json'\n",
    "# Create map with log number of avalanches per canton \n",
    "av_per_canton_map = folium.Map(location=[46.8, 8.239], zoom_start=8)\n",
    "av_per_canton_map.choropleth(geo_path = topo_path, data=avalanches_per_canton,\n",
    "                     columns=['canton', 'count'], \n",
    "                     key_on='feature.id',\n",
    "                     fill_color='PuRd', fill_opacity=0.7, line_opacity=0.2,\n",
    "                     threshold_scale=[0, 0.5, 1, 1.5, 2, 2.5], \n",
    "                     topojson='objects.cantons')\n",
    "helpers.addMarker(avalanches_per_community, av_per_canton_map, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Avalanches per canton__\n",
    "\n",
    "The markers are 30 places that had the most number of deadly avalanches the last 20 years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "av_per_canton_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a heat_map of avalanches\n",
    "avalanches_heat_map = folium.Map(location=[46.8, 8.239], zoom_start=8, tiles='Mapbox Bright', )\n",
    "avalanches_heat_map.choropleth(geo_path = topo_path, topojson='objects.cantons', fill_opacity=0.3, line_opacity=0.7), \n",
    "avalanches_heat_map_vals = avalanches_per_community.as_matrix()\n",
    "avalanches_heat_map.add_children(plugins.HeatMap(avalanches_heat_map_vals, radius = 17))\n",
    "helpers.addMarker(avalanches_per_community, avalanches_heat_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Avalanches heat map__\n",
    "\n",
    "The markers correspond to the 10 places with most number of avalanches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avalanches_heat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations and Remarks\n",
    "\n",
    "Most of the avalanches happen in the Bern, Valais and Grison canton. This is not too surprising since the Alpes are situated in those canton.\n",
    "From those three canton the highest density of avalanches happens in Valais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meteorological data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the meteorological data for the 10 places with the most avalanches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read the meteorological data\n",
    "meteo_data = pd.read_pickle('./data/meteo_data.pkl')\n",
    "#fix the index\n",
    "meteo_data = meteo_data.set_index([list(range(0, meteo_data.shape[0]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few of the features monthly and by winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_places = meteo_data.groupby(['Latitude', 'Longitude']).mean().shape[0]\n",
    "#since we're going to average over everything, we might as well drop the coordinates\n",
    "meteo_data_monthly = meteo_data.copy().drop(['Latitude', 'Longitude'], axis=1)\n",
    "#get rid of the day\n",
    "meteo_data_monthly['Date'] = meteo_data_monthly['Date'].map(lambda x: datetime.datetime(x.year, x.month, 1))\n",
    "#group by month, averaging most of the parameters but summing the snowfall and sunshine duration,\n",
    "#and averaging them by locations\n",
    "agg_functions = {}\n",
    "agg_functions.update(dict.fromkeys(meteo_data_monthly.columns.drop(['Date', 'Sunshine duration', 'Snowfall']), 'mean'))\n",
    "agg_functions.update(dict.fromkeys(['Sunshine duration', 'Snowfall'], lambda x: x.sum()/nb_places))\n",
    "meteo_data_monthly = meteo_data_monthly.groupby('Date', as_index=False).agg(agg_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replace the date by the month and the winter, respectively\n",
    "meteo_data_month = meteo_data_monthly.copy()\n",
    "#keep only the month\n",
    "meteo_data_month['Date'] = meteo_data_month['Date'].map(lambda x: x.month)\n",
    "#sort by month\n",
    "meteo_data_month.sort_values(by='Date', inplace=True)\n",
    "#replace the month by its name\n",
    "meteo_data_month['Date'] = meteo_data_month['Date'].map(lambda x: calendar.month_name[x])\n",
    "#group by month\n",
    "meteo_data_month = meteo_data_month.groupby('Date', sort=False).mean()\n",
    "\n",
    "meteo_data_winter = meteo_data_monthly.copy()\n",
    "#replace the date by the winter it belongs to\n",
    "meteo_data_winter['Date'] = meteo_data_winter['Date'].map(lambda x: helpers.date_to_winter(x))\n",
    "#group by winter and drop the first and the last since the data isn't complete\n",
    "meteo_data_winter = meteo_data_winter.groupby('Date').agg(agg_functions).drop(['1996-1997', '2016-2017'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the average snow depth, first by month then by winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "meteo_data_month.plot(y='Snow depth', yticks=np.arange(0, 1.1, 0.1), kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = meteo_data_winter.plot(y='Snow depth', yticks=np.arange(0, 0.51, 0.1))\n",
    "\n",
    "ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meteo_data_month.plot(y='Sunshine duration', yticks=range(0, 201, 10), kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No big surprise there. At least the data seems to make some sort of sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = meteo_data_winter.plot(y='Sunshine duration', yticks=range(194, 211, 1))\n",
    "ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Temperature](https://www.youtube.com/watch?v=dW2MmuA1nI4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meteo_data_month.plot(y='2 metre temperature', yticks=range(-5, 15), kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the winters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = meteo_data_winter.plot(y='2 metre temperature', yticks=np.arange(4, 7.1, 0.25))\n",
    "ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know a little more about our data, let's try to see if we can predict avalanches. First we need to prepare our data a little bit more: add a column saying if there was an avalanche that date or not, drop the useless columns (date and coordinates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_with_ts = data.copy().drop(['starting_zone_X', 'starting_zone_Y'], axis=1)\n",
    "data_with_ts['date_posix_ts'] = data_with_ts['date_posix_ts'].map(lambda date: date.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row = meteo_data.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_with_ts[(abs(data_with_ts['lat']-row['Latitude']) < 1e-2) \n",
    "         & (abs(data_with_ts['lon']-row['Longitude']) < 1e-2)]['date_posix_ts'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    lat = float(a.split('/')[1])+5e-4\n",
    "    lon = float(a.split('/')[0])+5e-4\n",
    "    return data_with_ts[(abs(data_with_ts['lat']-lat) < 5e-3) \n",
    "         & (abs(data_with_ts['lon']-lon) < 5e-3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f('7.939/47.004')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### def find_avalanche(row):\n",
    "    avalanche = data_with_ts[(abs(data_with_ts['lat']-row['Latitude']) < 5e-3) \n",
    "         & (abs(data_with_ts['lon']-row['Longitude']) < 5e-3)]\n",
    "    return pd.Series({'Avalanche': int((avalanche['date_posix_ts'] == row['Date'].timestamp()).values[0]),\n",
    "                         'Elevation': avalanche['elevation'].values[0],\n",
    "                         'Orientation': avalanche['aspect_string'].values[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meteo_data[['Avalanche', 'Elevation', 'Orientation']] = meteo_data.apply(find_avalanche, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meteo_data.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
